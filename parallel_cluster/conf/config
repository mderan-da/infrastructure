[aws]
aws_region_name = ap-southeast-2

[global]
cluster_template = tothill 
update_check     = true
sanity_check     = true

## Clusters
[cluster tothill]
base_os               = alinux2
key_name              = romanvg 
vpc_settings          = tothill_network
efs_settings          = awselasticfs
#fsx_settings          = lustrefs
s3_read_resource      = *
cluster_type          = spot
master_instance_type  = t2.medium
compute_instance_type = m5.large
#post_install          = s3://tothill-temp/parallel_cluster/testcluster/bootstrap.sh
#post_install_args     = "R wget"
ec2_iam_role          = parallelcluster-ec2-instance-role
scheduler             = slurm
initial_queue_size    = 1
custom_ami            = ami-002b1d0f440e6caea

[cluster umccr_dev]
base_os               = alinux2
vpc_settings          = umccr_dev_network
efs_settings          = awselasticfs
s3_read_resource      = *
cluster_type          = spot
key_name              = alexis-wfh-dev
# Need something substantial to hold the slurm database
master_instance_type  = t2.medium
# 16 CPUs and 64 Gb
compute_instance_type = m5.4xlarge
# Using additional_iam_policies over ec2_iam_role
# Add SSM policy, so we can keep port 22 closed
additional_iam_policies = arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
scheduler             = slurm
initial_queue_size    = 1
# Basic ami with installations of
# R, python3.8, conda, pip, ruby, golang, rust
# conda has been initialised to the ec2-user
# See the ami readme for recreation of the custom_ami
custom_ami            = ami-0a506aed533d1678d
pre_install           = s3://umccr-temp-dev/Alexis_parallel_cluster_test/bootstrap/pre_install.sh
post_install          = s3://umccr-temp-dev/Alexis_parallel_cluster_test/bootstrap/post_install.sh

[cluster umccr_dev_fsx]
base_os               = alinux2
vpc_settings          = umccr_dev_network
fsx_settings          = lustrefs
s3_read_resource      = *
cluster_type          = spot
key_name              = alexis-wfh-dev
# Need something substantial to hold the slurm database
master_instance_type  = t2.medium
# 16 CPUs and 64 Gb
compute_instance_type = m5.4xlarge
# Using additional_iam_policies over ec2_iam_role
# Add SSM policy, so we can keep port 22 closed
additional_iam_policies = arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
scheduler             = slurm
initial_queue_size    = 1
# Basic ami with installations of
# R, python3.8, conda, pip, ruby, golang, rust
# conda has been initialised to the ec2-user
# See the ami readme for recreation of the custom_ami
custom_ami            = ami-0a506aed533d1678d
pre_install           = s3://umccr-temp-dev/Alexis_parallel_cluster_test/bootstrap/pre_install.sh
post_install          = s3://umccr-temp-dev/Alexis_parallel_cluster_test/bootstrap/post_install.sh

## Networks
[vpc tothill_network]
vpc_id = vpc-0020eaf6bc27c98a4
master_subnet_id = subnet-07a7800b072d25eb3
compute_subnet_id = subnet-0506a40a906ea0156
use_public_ips = true

[vpc umccr_dev_network]
# Default vpc for dev account
vpc_id = vpc-00eafc63c0dfca266
# Compute subnet ids take the same subnet as the master
# Our default public subnet
master_subnet_id = subnet-0fab038b0341872f1
# Elastic IP address is associated to the master instance.
use_public_ips = true
# sg-0ca5bdaab39885649: Security group where the slurm accounting database sits
additional_sg         = sg-0ca5bdaab39885649

[aliases]
ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}

## Filesystems
[efs awselasticfs]
shared_dir       = efs
encrypted        = false
performance_mode = generalPurpose

[fsx lustrefs]
# The mountpoint of the filesystem
shared_dir                = /fsx
# Storage available in GB
storage_capacity          = 1200
# Default system
deployment_type           = SCRATCH_2
